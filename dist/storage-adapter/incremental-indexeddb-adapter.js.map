{
  "version": 3,
  "sources": ["../../src/storage-adapter/incremental-indexeddb-adapter.ts"],
  "sourcesContent": ["/* eslint-disable @typescript-eslint/no-this-alias */\nimport Collection from \"../database/collection\";\nimport { CollectionDocument } from \"../database/collection/collection-document\";\nimport Sylvie from \"../sylviejs\";\nimport { IncrementalPersistenceAdapter } from \"./src/models/persistence-adapter\";\nimport { PersistenceAdapterCallback } from \"./src/models/persistence-adapter-callback\";\n\nconst DEBUG =\n  // @ts-ignore\n  typeof window !== \"undefined\" && !!window.__loki_incremental_idb_debug;\n\ntype LokiIncrementalChunk = {\n  key: string;\n  index: number;\n  value: Partial<CollectionDocument>;\n  collectionName: string;\n  type: \"loki\";\n};\n\nexport type IncrementalChunk = (\n  | LokiIncrementalChunk\n  | {\n      key: string;\n      index: number;\n      value: Partial<CollectionDocument> | string;\n      collectionName: string;\n      dataChunks: LokiIncrementalChunk[];\n      type: \"data\";\n    }\n  | {\n      key: string;\n      index: number;\n      value: Partial<CollectionDocument>;\n      collectionName: string;\n      metadata: LokiIncrementalChunk;\n      dataChunks: LokiIncrementalChunk[];\n      type: \"metadata\";\n    }\n) &\n  Partial<CollectionDocument>;\n\nexport interface IncrementalIndexedDBAdapterOptions {\n  onversionchange?: (versionChangeEvent: IDBVersionChangeEvent) => void;\n  onFetchStart?: () => void;\n  onDidOverwrite?: () => void;\n  serializeChunk?: (\n    collectionName: string,\n    chunk: IncrementalChunk[],\n  ) => string;\n  serializeChunkAsync?: (\n    collectionName: string,\n    chunk: IncrementalChunk[],\n  ) => Promise<string>;\n  deserializeChunk?: (\n    collectionName: string,\n    chunkString: string,\n  ) => IncrementalChunk[];\n  deserializeChunkAsync?: (\n    collectionName: string,\n    chunkString: string,\n  ) => Promise<IncrementalChunk[]>;\n  megachunkCount?: number;\n  lazyCollections?: string[];\n}\n\n/**\n * An improved Loki persistence adapter for IndexedDB (not compatible with LokiIndexedAdapter)\n *     Unlike LokiIndexedAdapter, the database is saved not as one big JSON blob, but split into\n *     small chunks with individual collection documents. When saving, only the chunks with changed\n *     documents (and database metadata) is saved to IndexedDB. This speeds up small incremental\n *     saves by an order of magnitude on large (tens of thousands of records) databases. It also\n *     avoids Safari 13 bug that would cause the database to balloon in size to gigabytes\n *\n *     The `appname` argument is not provided - to distinguish between multiple app on the same\n *     domain, simply use a different Loki database name\n *\n * @example\n * var adapter = new IncrementalIndexedDBAdapter();\n *\n * @constructor IncrementalIndexedDBAdapter\n *\n * @param {object=} options Configuration options for the adapter\n * @param {function} options.onversionchange Function to call on `IDBDatabase.onversionchange` event\n *     (most likely database deleted from another browser tab)\n * @param {function} options.onFetchStart Function to call once IDB load has begun.\n *     Use this as an opportunity to execute code concurrently while IDB does work on a separate thread\n * @param {function} options.onDidOverwrite Called when this adapter is forced to overwrite contents\n *     of IndexedDB. This happens if there's another open tab of the same app that's making changes.\n *     You might use it as an opportunity to alert user to the potential loss of data\n * @param {function} options.(isData && isLazy))\n *  Called with a chunk (array of Loki documents) before\n *     it's saved to IndexedDB. You can use it to manually compress on-disk representation\n *     for faster database loads. Hint: Hand-written conversion of objects to arrays is very\n *     profitable for performance. If you use this, you must also pass options.deserializeChunk.\n * @param {function} options.deserializeChunk Called with a chunk serialized with options.serializeChunk\n *     Expects an array of Loki documents as the return value\n * @param {number} options.megachunkCount Number of parallel requests for data when loading database.\n *     Can be tuned for a specific application\n * @param {array} options.lazyCollections Names of collections that should be deserialized lazily\n *     Only use this for collections that aren't used at launch\n */\nexport class IncrementalIndexedDBAdapter\n  implements IncrementalPersistenceAdapter\n{\n  mode: \"incremental\";\n  options: Partial<IncrementalIndexedDBAdapterOptions>;\n  chunkSize: number;\n  megachunkCount: number;\n  lazyCollections: string[];\n  idb: null | IDBDatabase;\n  _prevLokiVersionId: string | null;\n  _prevCollectionVersionIds: {};\n  operationInProgress: any;\n  idbInitInProgress: any;\n\n  constructor(options?: Partial<IncrementalIndexedDBAdapterOptions>) {\n    this.mode = \"incremental\";\n    this.options = options || {};\n    this.chunkSize = 100;\n    this.megachunkCount = this.options.megachunkCount || 24;\n    this.lazyCollections = this.options.lazyCollections || [];\n    this.idb = null; // will be lazily loaded on first operation that needs it\n    this._prevLokiVersionId = null;\n    this._prevCollectionVersionIds = {};\n\n    if (!(this.megachunkCount >= 4 && this.megachunkCount % 2 === 0)) {\n      throw new Error(\"megachunkCount must be >=4 and divisible by 2\");\n    }\n\n    if (this.options.serializeChunk && !this.options.deserializeChunk) {\n      throw new Error(\n        \"serializeChunk requires deserializeChunk to be set as well\",\n      );\n    }\n\n    if (\n      [\n        !!this.options.serializeChunkAsync,\n        !!this.options.deserializeChunkAsync,\n      ].filter(Boolean).length === 1\n    ) {\n      throw new Error(\n        \"serializeChunkAsync requires deserializeChunkAsync to be set as well\",\n      );\n    }\n\n    // deserializeChunkAsync is not supported if lazyCollections is set\n    if (this.options.deserializeChunkAsync && this.lazyCollections.length > 0) {\n      throw new Error(\n        \"deserializeChunkAsync is not supported if lazyCollections is set\",\n      );\n    }\n  }\n\n  // chunkId - index of the data chunk - e.g. chunk 0 will be lokiIds 0-99\n  _getChunk(collection: Collection<IncrementalChunk>, chunkId: number) {\n    // 0-99, 100-199, etc.\n    const minId = chunkId * this.chunkSize;\n    const maxId = minId + this.chunkSize - 1;\n\n    // use idIndex to find first collection.data position within the $loki range\n    collection.ensureId();\n    const idIndex = collection.idIndex;\n\n    let firstDataPosition = null;\n\n    let max = idIndex.length - 1;\n    let min = 0;\n    let mid;\n\n    while (idIndex[min] < idIndex[max]) {\n      mid = (min + max) >> 1;\n\n      if (idIndex[mid] < minId) {\n        min = mid + 1;\n      } else {\n        max = mid;\n      }\n    }\n\n    if (max === min && idIndex[min] >= minId && idIndex[min] <= maxId) {\n      firstDataPosition = min;\n    }\n\n    if (firstDataPosition === null) {\n      // no elements in this chunk\n      return [];\n    }\n\n    // find last position\n    // if loki IDs are contiguous (no removed elements), last position will be first + chunk - 1\n    // (and we look back in case there are missing pieces)\n    // TODO: Binary search (not as important as first position, worst case scanario is only chunkSize steps)\n    let lastDataPosition = null;\n    for (\n      let i = firstDataPosition + this.chunkSize - 1;\n      i >= firstDataPosition;\n      i--\n    ) {\n      if (idIndex[i] <= maxId) {\n        lastDataPosition = i;\n        break;\n      }\n    }\n\n    // verify\n    const firstElement = collection.data[firstDataPosition];\n    if (\n      !(\n        firstElement &&\n        firstElement.$loki >= minId &&\n        firstElement.$loki <= maxId\n      )\n    ) {\n      throw new Error(\"broken invariant firstelement\");\n    }\n\n    const lastElement = collection.data[lastDataPosition];\n    if (\n      !(lastElement && lastElement.$loki >= minId && lastElement.$loki <= maxId)\n    ) {\n      throw new Error(\"broken invariant lastElement\");\n    }\n\n    // this will have *up to* 'this.chunkSize' elements (might have less, because $loki ids\n    // will have holes when data is deleted)\n    const chunkData = collection.data.slice(\n      firstDataPosition,\n      lastDataPosition + 1,\n    );\n\n    if (chunkData.length > this.chunkSize) {\n      throw new Error(\"broken invariant - chunk size\");\n    }\n\n    return chunkData;\n  }\n\n  /**\n   * Incrementally saves the database to IndexedDB\n   *\n   * @example\n   * var idbAdapter = new IncrementalIndexedDBAdapter();\n   * var db = new loki('test', { adapter: idbAdapter });\n   * var coll = db.addCollection('testColl');\n   * coll.insert({test: 'val'});\n   * db.saveDatabase();\n   *\n   * @param {string} dbname - the name to give the serialized database\n   * @param {function} getLokiCopy - returns copy of the Loki database\n   * @param {function} callback - (Optional) callback passed obj.success with true or false\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  saveDatabase = (\n    dbname: string,\n    getLokiCopy: () => Sylvie,\n    callback: PersistenceAdapterCallback,\n  ) => {\n    const that = this;\n\n    if (!this.idb) {\n      this._initializeIDBAsync(dbname)\n        .then(() => {\n          that.saveDatabase(dbname, getLokiCopy, callback);\n        })\n        .catch(callback);\n      return;\n    }\n\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while saving to database - another operation is already in progress. Please use throttledSaves=true option on Loki object\",\n      );\n    }\n    this.operationInProgress = true;\n\n    DEBUG && console.log(\"saveDatabase - begin\");\n    DEBUG && console.time(\"saveDatabase\");\n    function finish(e?: Error | null) {\n      DEBUG && e && console.error(e);\n      DEBUG && console.timeEnd(\"saveDatabase\");\n      that.operationInProgress = false;\n      callback(e);\n    }\n\n    // try..catch is required, e.g.:\n    // InvalidStateError: Failed to execute 'transaction' on 'IDBDatabase': The database connection is closing.\n    // (this may happen if another tab has called deleteDatabase)\n    try {\n      let updatePrevVersionIds = () => {\n        console.error(\n          \"Unexpected successful tx - cannot update previous version ids\",\n        );\n      };\n      let didOverwrite = false;\n\n      const tx = this.idb.transaction([\"LokiIncrementalData\"], \"readwrite\");\n      tx.oncomplete = () => {\n        updatePrevVersionIds();\n        finish();\n        if (didOverwrite && that.options.onDidOverwrite) {\n          that.options.onDidOverwrite();\n        }\n      };\n\n      tx.onerror = () => {\n        finish(tx.error);\n      };\n\n      tx.onabort = () => {\n        finish(tx.error);\n      };\n\n      const store = tx.objectStore(\"LokiIncrementalData\");\n\n      const performSave = (maxChunkIds?: Record<string, number>) => {\n        try {\n          const incremental = !maxChunkIds;\n          that\n            ._putInChunksAsync(store, getLokiCopy(), incremental, maxChunkIds)\n            .then((chunkInfo) => {\n              // Update last seen version IDs, but only after the transaction is successful\n              updatePrevVersionIds = () => {\n                that._prevLokiVersionId = chunkInfo.lokiVersionId;\n                chunkInfo.collectionVersionIds.forEach(\n                  ({ name, versionId }) => {\n                    that._prevCollectionVersionIds[name] = versionId;\n                  },\n                );\n              };\n              tx.commit && tx.commit();\n            });\n        } catch (error) {\n          console.error(\"idb performSave failed: \", error);\n          tx.abort();\n        }\n      };\n\n      // Incrementally saving changed chunks breaks down if there is more than one writer to IDB\n      // (multiple tabs of the same web app), leading to data corruption. To fix that, we save all\n      // metadata chunks (loki + collections) with a unique ID on each save and remember it. Before\n      // the subsequent save, we read loki from IDB to check if its version ID changed. If not, we're\n      // guaranteed that persisted DB is consistent with our diff. Otherwise, we fall back to the slow\n      // path and overwrite *all* database chunks with our version. Both reading and writing must\n      // happen in the same IDB transaction for this to work.\n      // TODO: We can optimize the slow path by fetching collection metadata chunks and comparing their\n      // version IDs with those last seen by us. Since any change in collection data requires a metadata\n      // chunk save, we're guaranteed that if the IDs match, we don't need to overwrite chukns of this collection\n      const getAllKeysThenSave = () => {\n        // NOTE: We must fetch all keys to protect against a case where another tab has wrote more\n        // chunks whan we did -- if so, we must delete them.\n        idbReqAsync(store.getAllKeys())\n          .then((result) => {\n            const maxChunkIds = getMaxChunkIds(result);\n            performSave(maxChunkIds);\n          })\n          .catch((e) => {\n            console.error(\"Getting all keys failed: \", e);\n            tx.abort();\n          });\n      };\n\n      const getLokiThenSave = () => {\n        idbReqAsync(store.get(\"loki\"))\n          .then((result) => {\n            if (lokiChunkVersionId(result) === that._prevLokiVersionId) {\n              performSave();\n            } else {\n              DEBUG &&\n                console.warn(\n                  \"Another writer changed Loki IDB, using slow path...\",\n                );\n              didOverwrite = true;\n              getAllKeysThenSave();\n            }\n          })\n          .catch((e) => {\n            console.error(\"Getting loki chunk failed: \", e);\n            tx.abort();\n          });\n      };\n\n      getLokiThenSave();\n    } catch (error) {\n      finish(error);\n    }\n  };\n\n  async _putInChunksAsync(\n    idbStore: IDBObjectStore,\n    loki: Sylvie & { idbVersionId?: string },\n    incremental: boolean,\n    maxChunkIds: Record<string, number>,\n  ) {\n    const that = this;\n    const collectionVersionIds = [];\n    let savedSize = 0;\n\n    const prepareCollection = async (collection, i) => {\n      // Find dirty chunk ids\n      const dirtyChunks = new Set();\n      incremental &&\n        collection.dirtyIds.forEach((lokiId) => {\n          const chunkId = (lokiId / that.chunkSize) | 0;\n          dirtyChunks.add(chunkId);\n        });\n      collection.dirtyIds = [];\n\n      // Serialize chunks to save\n      const prepareChunk = async (chunkId) => {\n        let chunkData: string | IncrementalChunk[] = that._getChunk(\n          collection,\n          chunkId,\n        );\n        if (that.options.serializeChunk) {\n          chunkData = that.options.serializeChunk(collection.name, chunkData);\n        } else if (that.options.serializeChunkAsync) {\n          chunkData = await that.options.serializeChunkAsync(\n            collection.name,\n            chunkData,\n          );\n        }\n        // we must stringify now, because IDB is asynchronous, and underlying objects are mutable\n        // In general, it's also faster to stringify, because we need serialization anyway, and\n        // JSON.stringify is much better optimized than IDB's structured clone\n        chunkData = JSON.stringify(chunkData);\n        savedSize += chunkData.length;\n        DEBUG &&\n          incremental &&\n          console.log(`Saving: ${collection.name}.chunk.${chunkId}`);\n        await idbStore.put({\n          key: `${collection.name}.chunk.${chunkId}`,\n          value: chunkData,\n        });\n      };\n      if (incremental) {\n        await Promise.all([...dirtyChunks].map(prepareChunk));\n      } else {\n        // add all chunks\n        const maxChunkId = (collection.maxId / that.chunkSize) | 0;\n        for (let j = 0; j <= maxChunkId; j += 1) {\n          await prepareChunk(j);\n        }\n\n        // delete chunks with larger ids than what we have\n        // NOTE: we don't have to delete metadata chunks as they will be absent from loki anyway\n        // NOTE: failures are silently ignored, so we don't have to worry about holes\n        const persistedMaxChunkId = maxChunkIds[collection.name] || 0;\n        for (let k = maxChunkId + 1; k <= persistedMaxChunkId; k += 1) {\n          const deletedChunkName = `${collection.name}.chunk.${k}`;\n          await idbStore.delete(deletedChunkName);\n          DEBUG && console.warn(`Deleted chunk: ${deletedChunkName}`);\n        }\n      }\n\n      // save collection metadata as separate chunk (but only if changed)\n      if (collection.dirty || dirtyChunks.size || !incremental) {\n        collection.idIndex = []; // this is recreated lazily\n        collection.data = [];\n        collection.idbVersionId = randomVersionId();\n        collectionVersionIds.push({\n          name: collection.name,\n          versionId: collection.idbVersionId,\n        });\n\n        const metadataChunk = JSON.stringify(collection);\n        savedSize += metadataChunk.length;\n        DEBUG &&\n          incremental &&\n          console.log(`Saving: ${collection.name}.metadata`);\n        await idbStore.put({\n          key: `${collection.name}.metadata`,\n          value: metadataChunk,\n        });\n      }\n\n      // leave only names in the loki chunk\n      loki.collections[i] = { name: collection.name } as Collection<\n        Partial<CollectionDocument>\n      >;\n    };\n    await Promise.all(loki.collections.map(prepareCollection));\n\n    loki.idbVersionId = randomVersionId();\n    const serializedMetadata = JSON.stringify(loki);\n    savedSize += serializedMetadata.length;\n\n    DEBUG && incremental && console.log(\"Saving: loki\");\n    await idbStore.put({ key: \"loki\", value: serializedMetadata });\n\n    DEBUG && console.log(`saved size: ${savedSize}`);\n    return {\n      lokiVersionId: loki.idbVersionId,\n      collectionVersionIds,\n    };\n  }\n\n  /**\n   * Retrieves a serialized db string from the catalog.\n   *\n   * @example\n   * // LOAD\n   * var idbAdapter = new IncrementalIndexedDBAdapter();\n   * var db = new loki('test', { adapter: idbAdapter });\n   * db.loadDatabase(function(result) {\n   *   console.log('done');\n   * });\n   *\n   * @param {string} dbname - the name of the database to retrieve.\n   * @param {function} callback - callback should accept string param containing serialized db string.\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  loadDatabase(dbname, callback) {\n    const that = this;\n\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while loading database - another operation is already in progress. Please use throttledSaves=true option on Loki object\",\n      );\n    }\n\n    this.operationInProgress = true;\n\n    DEBUG && console.log(\"loadDatabase - begin\");\n    DEBUG && console.time(\"loadDatabase\");\n\n    const finish = (value) => {\n      DEBUG && console.timeEnd(\"loadDatabase\");\n      that.operationInProgress = false;\n      callback(value);\n    };\n\n    this._getAllChunksAsync(dbname).then(async (chunks: any[]) => {\n      try {\n        if (!Array.isArray(chunks)) {\n          throw chunks; // we have an error\n        }\n\n        if (!chunks.length) {\n          return finish(null);\n        }\n\n        DEBUG && console.log(\"Found chunks:\", chunks.length);\n\n        // repack chunks into a map\n        let n_chunks = chunksToMap(chunks);\n        const loki = n_chunks.loki;\n        n_chunks.loki = null; // gc\n\n        // populate collections with data\n        populateLoki(\n          loki,\n          n_chunks.chunkMap,\n          that.options.deserializeChunk,\n          that.lazyCollections,\n        );\n        n_chunks = null; // gc\n\n        // remember previous version IDs\n        that._prevLokiVersionId = loki.idbVersionId || null;\n        that._prevCollectionVersionIds = {};\n        loki.collections.forEach(({ name, idbVersionId }) => {\n          that._prevCollectionVersionIds[name] = idbVersionId || null;\n        });\n\n        return finish(loki);\n      } catch (error) {\n        that._prevLokiVersionId = null;\n        that._prevCollectionVersionIds = {};\n        return finish(error);\n      }\n    });\n  }\n\n  async _initializeIDBAsync(dbname: string) {\n    return new Promise((resolve, reject) => {\n      this._initializeIDB(dbname, reject, resolve);\n    });\n  }\n\n  _initializeIDB(dbname: string, onError, onSuccess) {\n    const that = this;\n    DEBUG && console.log(\"initializing idb\");\n\n    if (this.idbInitInProgress) {\n      throw new Error(\n        \"Cannot open IndexedDB because open is already in progress\",\n      );\n    }\n    this.idbInitInProgress = true;\n\n    const openRequest = indexedDB.open(dbname, 1);\n\n    openRequest.onupgradeneeded = ({ oldVersion }) => {\n      const db = openRequest.result;\n      DEBUG && console.log(`onupgradeneeded, old version: ${oldVersion}`);\n\n      if (oldVersion < 1) {\n        // Version 1 - Initial - Create database\n        db.createObjectStore(\"LokiIncrementalData\", { keyPath: \"key\" });\n      } else {\n        // Unknown version\n        throw new Error(\n          `Invalid old version ${oldVersion} for IndexedDB upgrade`,\n        );\n      }\n    };\n\n    openRequest.onsuccess = () => {\n      that.idbInitInProgress = false;\n      const db = openRequest.result;\n      that.idb = db;\n\n      if (!db.objectStoreNames.contains(\"LokiIncrementalData\")) {\n        onError(new Error(\"Missing LokiIncrementalData\"));\n        // Attempt to recover (after reload) by deleting database, since it's damaged anyway\n        that.deleteDatabase(dbname);\n        return;\n      }\n\n      DEBUG && console.log(\"init success\");\n\n      db.onversionchange = (versionChangeEvent) => {\n        // Ignore if database was deleted and recreated in the meantime\n        if (that.idb !== db) {\n          return;\n        }\n\n        DEBUG && console.log(\"IDB version change\", versionChangeEvent);\n        // This function will be called if another connection changed DB version\n        // (Most likely database was deleted from another browser tab, unless there's a new version\n        // of this adapter, or someone makes a connection to IDB outside of this adapter)\n        // We must close the database to avoid blocking concurrent deletes.\n        // The database will be unusable after this. Be sure to supply `onversionchange` option\n        // to force logout\n        that.idb.close();\n        that.idb = null;\n        if (that.options.onversionchange) {\n          that.options.onversionchange(versionChangeEvent);\n        }\n      };\n\n      onSuccess();\n    };\n\n    openRequest.onblocked = (e) => {\n      console.error(\"IndexedDB open is blocked\", e);\n      onError(new Error(\"IndexedDB open is blocked by open connection\"));\n    };\n\n    openRequest.onerror = (e) => {\n      that.idbInitInProgress = false;\n      console.error(\"IndexedDB open error\", e);\n      onError(e);\n    };\n  }\n\n  async _getAllChunksAsync(dbname: string): Promise<any[]> {\n    const that = this;\n    return new Promise((resolve, reject) => {\n      if (!this.idb) {\n        this._initializeIDBAsync(dbname)\n          .then(() => {\n            that._getAllChunksAsync(dbname).then((result) => {\n              resolve(result);\n            });\n          })\n          .catch(reject);\n        return;\n      }\n\n      const tx = this.idb.transaction([\"LokiIncrementalData\"], \"readonly\");\n      const store = tx.objectStore(\"LokiIncrementalData\");\n\n      const deserializeChunkAsync = this.options.deserializeChunkAsync;\n      const deserializeChunk = this.options.deserializeChunk;\n      const lazyCollections = this.lazyCollections;\n\n      // If there are a lot of chunks (>100), don't request them all in one go, but in multiple\n      // \"megachunks\" (chunks of chunks). This improves concurrency, as main thread is already busy\n      // while IDB process is still fetching data. Details:\n\n      async function getMegachunksAsync(keys: IDBValidKey[]) {\n        const megachunkCount = that.megachunkCount;\n        const keyRanges = createKeyRanges(keys, megachunkCount);\n\n        const allChunks = [];\n        let megachunksReceived = 0;\n\n        async function processMegachunkAsync(\n          result,\n          megachunkIndex: number,\n          keyRange: { lower: number; upper: number },\n        ) {\n          const debugMsg =\n            \"processing chunk \" +\n            megachunkIndex +\n            \" (\" +\n            keyRange.lower +\n            \" -- \" +\n            keyRange.upper +\n            \")\";\n          DEBUG && console.time(debugMsg);\n          const megachunk = result;\n\n          for (const [i, chunk] of megachunk.entries()) {\n            await parseChunkAsync(\n              chunk,\n              deserializeChunkAsync,\n              deserializeChunk,\n              lazyCollections,\n            );\n            allChunks.push(chunk);\n            megachunk[i] = null; // gc\n          }\n\n          DEBUG && console.timeEnd(debugMsg);\n\n          megachunksReceived += 1;\n          if (megachunksReceived === megachunkCount) {\n            resolve(allChunks);\n          }\n        }\n\n        // Stagger megachunk requests - first one half, then request the second when first one comes\n        // back. This further improves concurrency.\n        const megachunkWaves = 2;\n        const megachunksPerWave = megachunkCount / megachunkWaves;\n        function requestMegachunk(index, wave) {\n          const keyRange = keyRanges[index];\n          idbReqAsync(store.getAll(keyRange))\n            .then(async (e) => {\n              if (wave < megachunkWaves) {\n                requestMegachunk(index + megachunksPerWave, wave + 1);\n              }\n\n              await processMegachunkAsync(e, index, keyRange);\n            })\n            .catch((e) => {\n              reject(e);\n            });\n        }\n\n        for (let i = 0; i < megachunksPerWave; i += 1) {\n          requestMegachunk(i, 1);\n        }\n      }\n\n      async function getAllChunksAsync() {\n        const result = await idbReqAsync(store.getAll());\n        const allChunks = result as any[];\n        for (const ch of allChunks) {\n          await parseChunkAsync(\n            ch,\n            deserializeChunkAsync,\n            deserializeChunk,\n            lazyCollections,\n          );\n        }\n        resolve(allChunks);\n      }\n\n      async function getAllKeysAsync() {\n        async function onDidGetKeysAsync(keys: IDBValidKey[]) {\n          keys.sort();\n          if (keys.length > 100) {\n            await getMegachunksAsync(keys);\n          } else {\n            await getAllChunksAsync();\n          }\n        }\n\n        idbReqAsync(store.getAllKeys())\n          .then(async (result) => {\n            await onDidGetKeysAsync(result as IDBValidKey[]);\n          })\n          .catch((e) => {\n            reject(e);\n          });\n\n        if (that.options.onFetchStart) {\n          that.options.onFetchStart();\n        }\n      }\n\n      getAllKeysAsync().then(() => {\n        return;\n      });\n    });\n  }\n\n  /**\n   * Deletes a database from IndexedDB\n   *\n   * @example\n   * // DELETE DATABASE\n   * // delete 'finance'/'test' value from catalog\n   * idbAdapter.deleteDatabase('test', function {\n   *   // database deleted\n   * });\n   *\n   * @param {string} dbname - the name of the database to delete from IDB\n   * @param {function=} callback - (Optional) executed on database delete\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  deleteDatabase(dbname: string, callback?: PersistenceAdapterCallback) {\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while deleting database - another operation is already in progress. Please use throttledSaves=true option on Loki object\",\n      );\n    }\n\n    this.operationInProgress = true;\n\n    const that = this;\n    DEBUG && console.log(\"deleteDatabase - begin\");\n    DEBUG && console.time(\"deleteDatabase\");\n\n    this._prevLokiVersionId = null;\n    this._prevCollectionVersionIds = {};\n\n    if (this.idb) {\n      this.idb.close();\n      this.idb = null;\n    }\n\n    const request = indexedDB.deleteDatabase(dbname);\n\n    request.onsuccess = () => {\n      that.operationInProgress = false;\n      DEBUG && console.timeEnd(\"deleteDatabase\");\n      callback({ success: true });\n    };\n\n    request.onerror = (e) => {\n      that.operationInProgress = false;\n      console.error(\"Error while deleting database\", e);\n      callback({ success: false, error: request.error });\n    };\n\n    request.onblocked = (e) => {\n      // We can't call callback with failure status, because this will be called even if we\n      // succeed in just a moment\n      console.error(\n        \"Deleting database failed because it's blocked by another connection\",\n        e,\n      );\n    };\n  }\n}\n\n// gets current largest chunk ID for each collection\nfunction getMaxChunkIds(allKeys) {\n  const maxChunkIds = {};\n\n  allKeys.forEach((key) => {\n    const keySegments = key.split(\".\");\n    // table.chunk.2317\n    if (keySegments.length === 3 && keySegments[1] === \"chunk\") {\n      const collection = keySegments[0];\n      const chunkId = parseInt(keySegments[2]) || 0;\n      const currentMax = maxChunkIds[collection];\n\n      if (!currentMax || chunkId > currentMax) {\n        maxChunkIds[collection] = chunkId;\n      }\n    }\n  });\n  return maxChunkIds;\n}\n\nfunction lokiChunkVersionId(chunk) {\n  try {\n    if (chunk) {\n      const loki = JSON.parse(chunk.value);\n      return loki.idbVersionId || null;\n    } else {\n      return null;\n    }\n  } catch (e) {\n    console.error(\"Error while parsing loki chunk\", e);\n    return null;\n  }\n}\n\nfunction chunksToMap(chunks: IncrementalChunk[]): {\n  loki: any;\n  chunkMap: Record<string, IncrementalChunk>;\n} {\n  let loki;\n  const chunkMap = {};\n\n  sortChunksInPlace(chunks);\n\n  chunks.forEach((chunk) => {\n    const type = chunk.type;\n    const value = chunk.value;\n    const name = chunk.collectionName;\n    if (type === \"loki\") {\n      loki = value;\n    } else if (type === \"data\") {\n      if (chunkMap[name]) {\n        chunkMap[name].dataChunks.push(value);\n      } else {\n        chunkMap[name] = {\n          metadata: null,\n          dataChunks: [value],\n        };\n      }\n    } else if (type === \"metadata\") {\n      if (chunkMap[name]) {\n        chunkMap[name].metadata = value;\n      } else {\n        chunkMap[name] = { metadata: value, dataChunks: [] };\n      }\n    } else {\n      throw new Error(\"unreachable\");\n    }\n  });\n\n  if (!loki) {\n    throw new Error(\"Corrupted database - missing database metadata\");\n  }\n\n  return { loki, chunkMap };\n}\n\n// async function populateLokiAsync(\n//   {\n//     collections,\n//   }: {\n//     collections: Collection<IncrementalChunk & Partial<CollectionDocument>>[];\n//   },\n//   chunkMap: Record<string, IncrementalChunk>,\n//   deserializeChunkAsync: (\n//     collectionName: string,\n//     chunk: any,\n//   ) => Promise<IncrementalChunk[]>,\n//   lazyCollections: string[],\n// ) {\n//   console.log(\"  populateLokiAsync called\");\n//   const promises: Promise<unknown>[] = [];\n//   collections.map(async (collectionStub, i) => {\n//     const name = collectionStub.name;\n//     const chunkCollection = chunkMap[name];\n//     console.log(\"CHUNKMAP\");\n//     console.log(chunkCollection);\n//     if (chunkCollection) {\n//       if (!chunkCollection.metadata) {\n//         throw new Error(\n//           `Corrupted database - missing metadata chunk for ${name}`,\n//         );\n//       }\n//       const collection = chunkCollection.metadata;\n//       chunkCollection.metadata = null;\n//       collections[i] = collection;\n\n//       const isLazy = lazyCollections.includes(name);\n//       const lokiDeserializeCollectionChunks = () => {\n//         DEBUG && isLazy && console.log(`lazy loading ${name}`);\n//         const data = [];\n//         const dataChunks = chunkCollection.dataChunks;\n\n//         // for (const [chunk, i] of dataChunks.entries()) {\n//         // }\n//         dataChunks.forEach(function populateChunk(chunk, i) {\n//           if (isLazy) {\n//             chunk = JSON.parse(chunk);\n//             if (deserializeChunkAsync) {\n//               const prom = () =>\n//                 deserializeChunkAsync(name, chunk).then((result) => {\n//                   // May be race condition? return before callback\n//                   result.forEach((doc) => {\n//                     data.push(doc);\n//                   });\n//                   dataChunks[i] = null;\n//                 });\n//               promises.push(prom());\n//             }\n//           } else {\n//             chunk.forEach((doc) => {\n//               data.push(doc);\n//             });\n//             dataChunks[i] = null;\n//           }\n//         });\n//         return data;\n//       };\n//       collection.getData = lokiDeserializeCollectionChunks;\n//     }\n//   });\n//   await Promise.all(promises);\n//   console.log(\"  populateLokiAsync finished awaited promises\");\n// }\n\nfunction populateLoki(\n  {\n    collections,\n  }: {\n    collections: Collection<IncrementalChunk & Partial<CollectionDocument>>[];\n  },\n  chunkMap: Record<string, any>,\n  deserializeChunk: (collectionName: string, chunk: any) => any,\n  lazyCollections: string[],\n) {\n  collections.forEach(function populateCollection(collectionStub, i) {\n    const name = collectionStub.name;\n    const chunkCollection = chunkMap[name];\n    if (chunkCollection) {\n      if (!chunkCollection.metadata) {\n        throw new Error(\n          `Corrupted database - missing metadata chunk for ${name}`,\n        );\n      }\n      const collection = chunkCollection.metadata;\n      chunkCollection.metadata = null;\n      collections[i] = collection;\n\n      const isLazy = lazyCollections.includes(name);\n      const lokiDeserializeCollectionChunks = () => {\n        DEBUG && isLazy && console.log(`lazy loading ${name}`);\n        const data = [];\n        const dataChunks = chunkCollection.dataChunks;\n        dataChunks.forEach(function populateChunk(chunk, i) {\n          if (isLazy) {\n            chunk = JSON.parse(chunk);\n            if (deserializeChunk) {\n              chunk = deserializeChunk(name, chunk);\n            }\n          }\n          chunk.forEach((doc) => {\n            data.push(doc);\n          });\n          dataChunks[i] = null;\n        });\n        return data;\n      };\n      collection.getData = lokiDeserializeCollectionChunks;\n    }\n  });\n}\n\nfunction classifyChunk(chunk: IncrementalChunk) {\n  const key = chunk.key;\n\n  if (key === \"loki\") {\n    chunk.type = \"loki\";\n    return;\n  } else if (key.includes(\".\")) {\n    const keySegments = key.split(\".\");\n    if (keySegments.length === 3 && keySegments[1] === \"chunk\") {\n      chunk.type = \"data\";\n      chunk.collectionName = keySegments[0];\n      chunk.index = parseInt(keySegments[2], 10);\n      return;\n    } else if (keySegments.length === 2 && keySegments[1] === \"metadata\") {\n      chunk.type = \"metadata\";\n      chunk.collectionName = keySegments[0];\n      return;\n    }\n  }\n\n  console.error(`Unknown chunk ${key}`);\n  throw new Error(\"Corrupted database - unknown chunk found\");\n}\n\n// rewrite parseChunk to use Async and deserializeChunkAsync\nasync function parseChunkAsync(\n  chunk: IncrementalChunk,\n  deserializeChunkAsync: (\n    collectionName: string,\n    chunkString: string,\n  ) => Promise<IncrementalChunk[]>,\n  deserializeChunk: (\n    collectionName: string,\n    chunkString: string,\n  ) => IncrementalChunk[],\n  lazyCollections,\n) {\n  classifyChunk(chunk);\n\n  const isData = chunk.type === \"data\";\n  const isLazy = lazyCollections.includes(chunk.collectionName);\n\n  if (!(isData && isLazy)) {\n    chunk.value = JSON.parse(chunk.value as string);\n  }\n  if (isData && !isLazy) {\n    if (deserializeChunk) {\n      chunk.value = deserializeChunk(\n        chunk.collectionName,\n        chunk.value as string,\n      );\n    } else if (deserializeChunkAsync) {\n      chunk.value = await deserializeChunkAsync(\n        chunk.collectionName,\n        chunk.value as string,\n      );\n    }\n  }\n}\n\nfunction randomVersionId(): string {\n  // Appears to have enough entropy for chunk version IDs\n  // (Only has to be different than enough of its own previous versions that there's no writer\n  // that thinks a new version is the same as an earlier one, not globally unique)\n  return Math.random().toString(36).substring(2);\n}\n\nfunction sortChunksInPlace(chunks: IncrementalChunk[]) {\n  // sort chunks in place to load data in the right order (ascending loki ids)\n  // on both Safari and Chrome, we'll get chunks in order like this: 0, 1, 10, 100...\n  chunks.sort(function (a, b) {\n    return (a.index || 0) - (b.index || 0);\n  });\n}\n\nfunction createKeyRanges(keys: IDBValidKey[], count: number) {\n  const countPerRange = Math.floor(keys.length / count);\n  const keyRanges = [];\n  let minKey;\n  let maxKey;\n  for (let i = 0; i < count; i += 1) {\n    minKey = keys[countPerRange * i];\n    maxKey = keys[countPerRange * (i + 1)];\n    if (i === 0) {\n      // ... < maxKey\n      keyRanges.push(IDBKeyRange.upperBound(maxKey, true));\n    } else if (i === count - 1) {\n      // >= minKey\n      keyRanges.push(IDBKeyRange.lowerBound(minKey));\n    } else {\n      // >= minKey && < maxKey\n      keyRanges.push(IDBKeyRange.bound(minKey, maxKey, false, true));\n    }\n  }\n  return keyRanges;\n}\n\nasync function idbReqAsync<T>(request: IDBRequest<T>) {\n  return new Promise((resolve, reject) => {\n    request.onsuccess = (e) => {\n      resolve((e.target as any).result as T);\n    };\n    request.onerror = () => {\n      reject(request.error);\n    };\n  });\n}\n\nif (window !== undefined) {\n  Object.assign(window, { IncrementalIndexedDBAdapter });\n}\n"],
  "mappings": "4RAOA,IAAMA,EAEJ,OAAO,QAAW,aAAe,CAAC,CAAC,OAAO,6BA4F/BC,EAAN,KAEP,CAYE,YAAYC,EAAuD,CA0InE,kBAAeC,EAAA,CACbC,EACAC,EACAC,IACG,CACH,IAAMC,EAAO,KAEb,GAAI,CAAC,KAAK,IAAK,CACb,KAAK,oBAAoBH,CAAM,EAC5B,KAAK,IAAM,CACVG,EAAK,aAAaH,EAAQC,EAAaC,CAAQ,CACjD,CAAC,EACA,MAAMA,CAAQ,EACjB,OAGF,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,iIACF,EAEF,KAAK,oBAAsB,GAE3BN,GAAS,QAAQ,IAAI,sBAAsB,EAC3CA,GAAS,QAAQ,KAAK,cAAc,EACpC,SAASQ,EAAO,EAAkB,CAChCR,GAAS,GAAK,QAAQ,MAAM,CAAC,EAC7BA,GAAS,QAAQ,QAAQ,cAAc,EACvCO,EAAK,oBAAsB,GAC3BD,EAAS,CAAC,CACZ,CALSH,EAAAK,EAAA,UAUT,GAAI,CACF,IAAIC,EAAuBN,EAAA,IAAM,CAC/B,QAAQ,MACN,+DACF,CACF,EAJ2B,wBAKvBO,EAAe,GAEbC,EAAK,KAAK,IAAI,YAAY,CAAC,qBAAqB,EAAG,WAAW,EACpEA,EAAG,WAAa,IAAM,CACpBF,EAAqB,EACrBD,EAAO,EACHE,GAAgBH,EAAK,QAAQ,gBAC/BA,EAAK,QAAQ,eAAe,CAEhC,EAEAI,EAAG,QAAU,IAAM,CACjBH,EAAOG,EAAG,KAAK,CACjB,EAEAA,EAAG,QAAU,IAAM,CACjBH,EAAOG,EAAG,KAAK,CACjB,EAEA,IAAMC,EAAQD,EAAG,YAAY,qBAAqB,EAE5CE,EAAcV,EAACW,GAAyC,CAC5D,GAAI,CACF,IAAMC,EAAc,CAACD,EACrBP,EACG,kBAAkBK,EAAOP,EAAY,EAAGU,EAAaD,CAAW,EAChE,KAAME,GAAc,CAEnBP,EAAuBN,EAAA,IAAM,CAC3BI,EAAK,mBAAqBS,EAAU,cACpCA,EAAU,qBAAqB,QAC7B,CAAC,CAAE,KAAAC,EAAM,UAAAC,CAAU,IAAM,CACvBX,EAAK,0BAA0BU,CAAI,EAAIC,CACzC,CACF,CACF,EAPuB,wBAQvBP,EAAG,QAAUA,EAAG,OAAO,CACzB,CAAC,CACL,OAASQ,EAAP,CACA,QAAQ,MAAM,2BAA4BA,CAAK,EAC/CR,EAAG,MAAM,CACX,CACF,EArBoB,eAiCdS,EAAqBjB,EAAA,IAAM,CAG/BkB,EAAYT,EAAM,WAAW,CAAC,EAC3B,KAAMU,GAAW,CAChB,IAAMR,EAAcS,EAAeD,CAAM,EACzCT,EAAYC,CAAW,CACzB,CAAC,EACA,MAAOU,GAAM,CACZ,QAAQ,MAAM,4BAA6BA,CAAC,EAC5Cb,EAAG,MAAM,CACX,CAAC,CACL,EAZ2B,sBAcHR,EAAA,IAAM,CAC5BkB,EAAYT,EAAM,IAAI,MAAM,CAAC,EAC1B,KAAMU,GAAW,CACZG,EAAmBH,CAAM,IAAMf,EAAK,mBACtCM,EAAY,GAEZb,GACE,QAAQ,KACN,qDACF,EACFU,EAAe,GACfU,EAAmB,EAEvB,CAAC,EACA,MAAOI,GAAM,CACZ,QAAQ,MAAM,8BAA+BA,CAAC,EAC9Cb,EAAG,MAAM,CACX,CAAC,CACL,EAlBwB,mBAoBR,CAClB,OAASQ,EAAP,CACAX,EAAOW,CAAK,CACd,CACF,EArIe,gBAhIb,GATA,KAAK,KAAO,cACZ,KAAK,QAAUjB,GAAW,CAAC,EAC3B,KAAK,UAAY,IACjB,KAAK,eAAiB,KAAK,QAAQ,gBAAkB,GACrD,KAAK,gBAAkB,KAAK,QAAQ,iBAAmB,CAAC,EACxD,KAAK,IAAM,KACX,KAAK,mBAAqB,KAC1B,KAAK,0BAA4B,CAAC,EAE9B,EAAE,KAAK,gBAAkB,GAAK,KAAK,eAAiB,IAAM,GAC5D,MAAM,IAAI,MAAM,+CAA+C,EAGjE,GAAI,KAAK,QAAQ,gBAAkB,CAAC,KAAK,QAAQ,iBAC/C,MAAM,IAAI,MACR,4DACF,EAGF,GACE,CACE,CAAC,CAAC,KAAK,QAAQ,oBACf,CAAC,CAAC,KAAK,QAAQ,qBACjB,EAAE,OAAO,OAAO,EAAE,SAAW,EAE7B,MAAM,IAAI,MACR,sEACF,EAIF,GAAI,KAAK,QAAQ,uBAAyB,KAAK,gBAAgB,OAAS,EACtE,MAAM,IAAI,MACR,kEACF,CAEJ,CAGA,UAAUwB,EAA0CC,EAAiB,CAEnE,IAAMC,EAAQD,EAAU,KAAK,UACvBE,EAAQD,EAAQ,KAAK,UAAY,EAGvCF,EAAW,SAAS,EACpB,IAAMI,EAAUJ,EAAW,QAEvBK,EAAoB,KAEpBC,EAAMF,EAAQ,OAAS,EACvBG,EAAM,EACNC,EAEJ,KAAOJ,EAAQG,CAAG,EAAIH,EAAQE,CAAG,GAC/BE,EAAOD,EAAMD,GAAQ,EAEjBF,EAAQI,CAAG,EAAIN,EACjBK,EAAMC,EAAM,EAEZF,EAAME,EAQV,GAJIF,IAAQC,GAAOH,EAAQG,CAAG,GAAKL,GAASE,EAAQG,CAAG,GAAKJ,IAC1DE,EAAoBE,GAGlBF,IAAsB,KAExB,MAAO,CAAC,EAOV,IAAII,EAAmB,KACvB,QACMC,EAAIL,EAAoB,KAAK,UAAY,EAC7CK,GAAKL,EACLK,IAEA,GAAIN,EAAQM,CAAC,GAAKP,EAAO,CACvBM,EAAmBC,EACnB,MAKJ,IAAMC,EAAeX,EAAW,KAAKK,CAAiB,EACtD,GACE,EACEM,GACAA,EAAa,OAAST,GACtBS,EAAa,OAASR,GAGxB,MAAM,IAAI,MAAM,+BAA+B,EAGjD,IAAMS,EAAcZ,EAAW,KAAKS,CAAgB,EACpD,GACE,EAAEG,GAAeA,EAAY,OAASV,GAASU,EAAY,OAAST,GAEpE,MAAM,IAAI,MAAM,8BAA8B,EAKhD,IAAMU,EAAYb,EAAW,KAAK,MAChCK,EACAI,EAAmB,CACrB,EAEA,GAAII,EAAU,OAAS,KAAK,UAC1B,MAAM,IAAI,MAAM,+BAA+B,EAGjD,OAAOA,CACT,CAwJM,kBACJC,EACAC,EACA1B,EACAD,EACA,QAAA4B,EAAA,sBACA,IAAMnC,EAAO,KACPoC,EAAuB,CAAC,EAC1BC,EAAY,EAEVC,EAAoB1C,EAAA,CAAOuB,EAAYU,IAAMM,EAAA,sBAEjD,IAAMI,EAAc,IAAI,IACxB/B,GACEW,EAAW,SAAS,QAASqB,GAAW,CACtC,IAAMpB,EAAWoB,EAASxC,EAAK,UAAa,EAC5CuC,EAAY,IAAInB,CAAO,CACzB,CAAC,EACHD,EAAW,SAAW,CAAC,EAGvB,IAAMsB,EAAe7C,EAAOwB,GAAYe,EAAA,sBACtC,IAAIH,EAAyChC,EAAK,UAChDmB,EACAC,CACF,EACIpB,EAAK,QAAQ,eACfgC,EAAYhC,EAAK,QAAQ,eAAemB,EAAW,KAAMa,CAAS,EACzDhC,EAAK,QAAQ,sBACtBgC,EAAY,MAAMhC,EAAK,QAAQ,oBAC7BmB,EAAW,KACXa,CACF,GAKFA,EAAY,KAAK,UAAUA,CAAS,EACpCK,GAAaL,EAAU,OACvBvC,GACEe,GACA,QAAQ,IAAI,WAAWW,EAAW,cAAcC,GAAS,EAC3D,MAAMa,EAAS,IAAI,CACjB,IAAK,GAAGd,EAAW,cAAcC,IACjC,MAAOY,CACT,CAAC,CACH,GAzBqB,gBA0BrB,GAAIxB,EACF,MAAM,QAAQ,IAAI,CAAC,GAAG+B,CAAW,EAAE,IAAIE,CAAY,CAAC,MAC/C,CAEL,IAAMC,EAAcvB,EAAW,MAAQnB,EAAK,UAAa,EACzD,QAAS2C,EAAI,EAAGA,GAAKD,EAAYC,GAAK,EACpC,MAAMF,EAAaE,CAAC,EAMtB,IAAMC,EAAsBrC,EAAYY,EAAW,IAAI,GAAK,EAC5D,QAAS0B,EAAIH,EAAa,EAAGG,GAAKD,EAAqBC,GAAK,EAAG,CAC7D,IAAMC,EAAmB,GAAG3B,EAAW,cAAc0B,IACrD,MAAMZ,EAAS,OAAOa,CAAgB,EACtCrD,GAAS,QAAQ,KAAK,kBAAkBqD,GAAkB,GAK9D,GAAI3B,EAAW,OAASoB,EAAY,MAAQ,CAAC/B,EAAa,CACxDW,EAAW,QAAU,CAAC,EACtBA,EAAW,KAAO,CAAC,EACnBA,EAAW,aAAe4B,EAAgB,EAC1CX,EAAqB,KAAK,CACxB,KAAMjB,EAAW,KACjB,UAAWA,EAAW,YACxB,CAAC,EAED,IAAM6B,EAAgB,KAAK,UAAU7B,CAAU,EAC/CkB,GAAaW,EAAc,OAC3BvD,GACEe,GACA,QAAQ,IAAI,WAAWW,EAAW,eAAe,EACnD,MAAMc,EAAS,IAAI,CACjB,IAAK,GAAGd,EAAW,gBACnB,MAAO6B,CACT,CAAC,EAIHd,EAAK,YAAYL,CAAC,EAAI,CAAE,KAAMV,EAAW,IAAK,CAGhD,GAlF0B,qBAmF1B,MAAM,QAAQ,IAAIe,EAAK,YAAY,IAAII,CAAiB,CAAC,EAEzDJ,EAAK,aAAea,EAAgB,EACpC,IAAME,EAAqB,KAAK,UAAUf,CAAI,EAC9C,OAAAG,GAAaY,EAAmB,OAEhCxD,GAASe,GAAe,QAAQ,IAAI,cAAc,EAClD,MAAMyB,EAAS,IAAI,CAAE,IAAK,OAAQ,MAAOgB,CAAmB,CAAC,EAE7DxD,GAAS,QAAQ,IAAI,eAAe4C,GAAW,EACxC,CACL,cAAeH,EAAK,aACpB,qBAAAE,CACF,CACF,GAiBA,aAAavC,EAAQE,EAAU,CAC7B,IAAMC,EAAO,KAEb,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,+HACF,EAGF,KAAK,oBAAsB,GAE3BP,GAAS,QAAQ,IAAI,sBAAsB,EAC3CA,GAAS,QAAQ,KAAK,cAAc,EAEpC,IAAMQ,EAASL,EAACsD,GAAU,CACxBzD,GAAS,QAAQ,QAAQ,cAAc,EACvCO,EAAK,oBAAsB,GAC3BD,EAASmD,CAAK,CAChB,EAJe,UAMf,KAAK,mBAAmBrD,CAAM,EAAE,KAAYsD,GAAkBhB,EAAA,sBAC5D,GAAI,CACF,GAAI,CAAC,MAAM,QAAQgB,CAAM,EACvB,MAAMA,EAGR,GAAI,CAACA,EAAO,OACV,OAAOlD,EAAO,IAAI,EAGpBR,GAAS,QAAQ,IAAI,gBAAiB0D,EAAO,MAAM,EAGnD,IAAIC,EAAWC,EAAYF,CAAM,EAC3BjB,EAAOkB,EAAS,KACtB,OAAAA,EAAS,KAAO,KAGhBE,EACEpB,EACAkB,EAAS,SACTpD,EAAK,QAAQ,iBACbA,EAAK,eACP,EACAoD,EAAW,KAGXpD,EAAK,mBAAqBkC,EAAK,cAAgB,KAC/ClC,EAAK,0BAA4B,CAAC,EAClCkC,EAAK,YAAY,QAAQ,CAAC,CAAE,KAAAxB,EAAM,aAAA6C,CAAa,IAAM,CACnDvD,EAAK,0BAA0BU,CAAI,EAAI6C,GAAgB,IACzD,CAAC,EAEMtD,EAAOiC,CAAI,CACpB,OAAStB,EAAP,CACA,OAAAZ,EAAK,mBAAqB,KAC1BA,EAAK,0BAA4B,CAAC,EAC3BC,EAAOW,CAAK,CACrB,CACF,EAAC,CACH,CAEM,oBAAoBf,EAAgB,QAAAsC,EAAA,sBACxC,OAAO,IAAI,QAAQ,CAACqB,EAASC,IAAW,CACtC,KAAK,eAAe5D,EAAQ4D,EAAQD,CAAO,CAC7C,CAAC,CACH,GAEA,eAAe3D,EAAgB6D,EAASC,EAAW,CACjD,IAAM3D,EAAO,KAGb,GAFAP,GAAS,QAAQ,IAAI,kBAAkB,EAEnC,KAAK,kBACP,MAAM,IAAI,MACR,2DACF,EAEF,KAAK,kBAAoB,GAEzB,IAAMmE,EAAc,UAAU,KAAK/D,EAAQ,CAAC,EAE5C+D,EAAY,gBAAkB,CAAC,CAAE,WAAAC,CAAW,IAAM,CAChD,IAAMC,EAAKF,EAAY,OAGvB,GAFAnE,GAAS,QAAQ,IAAI,iCAAiCoE,GAAY,EAE9DA,EAAa,EAEfC,EAAG,kBAAkB,sBAAuB,CAAE,QAAS,KAAM,CAAC,MAG9D,OAAM,IAAI,MACR,uBAAuBD,yBACzB,CAEJ,EAEAD,EAAY,UAAY,IAAM,CAC5B5D,EAAK,kBAAoB,GACzB,IAAM8D,EAAKF,EAAY,OAGvB,GAFA5D,EAAK,IAAM8D,EAEP,CAACA,EAAG,iBAAiB,SAAS,qBAAqB,EAAG,CACxDJ,EAAQ,IAAI,MAAM,6BAA6B,CAAC,EAEhD1D,EAAK,eAAeH,CAAM,EAC1B,OAGFJ,GAAS,QAAQ,IAAI,cAAc,EAEnCqE,EAAG,gBAAmBC,GAAuB,CAEvC/D,EAAK,MAAQ8D,IAIjBrE,GAAS,QAAQ,IAAI,qBAAsBsE,CAAkB,EAO7D/D,EAAK,IAAI,MAAM,EACfA,EAAK,IAAM,KACPA,EAAK,QAAQ,iBACfA,EAAK,QAAQ,gBAAgB+D,CAAkB,EAEnD,EAEAJ,EAAU,CACZ,EAEAC,EAAY,UAAa,GAAM,CAC7B,QAAQ,MAAM,4BAA6B,CAAC,EAC5CF,EAAQ,IAAI,MAAM,8CAA8C,CAAC,CACnE,EAEAE,EAAY,QAAW,GAAM,CAC3B5D,EAAK,kBAAoB,GACzB,QAAQ,MAAM,uBAAwB,CAAC,EACvC0D,EAAQ,CAAC,CACX,CACF,CAEM,mBAAmB7D,EAAgC,QAAAsC,EAAA,sBACvD,IAAMnC,EAAO,KACb,OAAO,IAAI,QAAQ,CAACwD,EAASC,IAAW,CACtC,GAAI,CAAC,KAAK,IAAK,CACb,KAAK,oBAAoB5D,CAAM,EAC5B,KAAK,IAAM,CACVG,EAAK,mBAAmBH,CAAM,EAAE,KAAMkB,GAAW,CAC/CyC,EAAQzC,CAAM,CAChB,CAAC,CACH,CAAC,EACA,MAAM0C,CAAM,EACf,OAIF,IAAMpD,EADK,KAAK,IAAI,YAAY,CAAC,qBAAqB,EAAG,UAAU,EAClD,YAAY,qBAAqB,EAE5C2D,EAAwB,KAAK,QAAQ,sBACrCC,EAAmB,KAAK,QAAQ,iBAChCC,EAAkB,KAAK,gBAM7B,SAAeC,EAAmBC,EAAqB,QAAAjC,EAAA,sBACrD,IAAMkC,EAAiBrE,EAAK,eACtBsE,EAAYC,EAAgBH,EAAMC,CAAc,EAEhDG,EAAY,CAAC,EACfC,EAAqB,EAEzB,SAAeC,EACb3D,EACA4D,EACAC,EACA,QAAAzC,EAAA,sBACA,IAAM0C,EACJ,oBACAF,EACA,KACAC,EAAS,MACT,OACAA,EAAS,MACT,IACFnF,GAAS,QAAQ,KAAKoF,CAAQ,EAC9B,IAAMC,EAAY/D,EAElB,OAAW,CAACc,EAAGkD,CAAK,IAAKD,EAAU,QAAQ,EACzC,MAAME,EACJD,EACAf,EACAC,EACAC,CACF,EACAM,EAAU,KAAKO,CAAK,EACpBD,EAAUjD,CAAC,EAAI,KAGjBpC,GAAS,QAAQ,QAAQoF,CAAQ,EAEjCJ,GAAsB,EAClBA,IAAuBJ,GACzBb,EAAQgB,CAAS,CAErB,GAjCe5E,EAAA8E,EAAA,yBAqCf,IAAMO,EAAiB,EACjBC,EAAoBb,EAAiBY,EAC3C,SAASE,EAAiBC,EAAOC,EAAM,CACrC,IAAMT,EAAWN,EAAUc,CAAK,EAChCtE,EAAYT,EAAM,OAAOuE,CAAQ,CAAC,EAC/B,KAAY3D,GAAMkB,EAAA,sBACbkD,EAAOJ,GACTE,EAAiBC,EAAQF,EAAmBG,EAAO,CAAC,EAGtD,MAAMX,EAAsBzD,EAAGmE,EAAOR,CAAQ,CAChD,EAAC,EACA,MAAO3D,GAAM,CACZwC,EAAOxC,CAAC,CACV,CAAC,CACL,CAbSrB,EAAAuF,EAAA,oBAeT,QAAStD,EAAI,EAAGA,EAAIqD,EAAmBrD,GAAK,EAC1CsD,EAAiBtD,EAAG,CAAC,CAEzB,GAhEejC,EAAAuE,EAAA,sBAkEf,SAAemB,GAAoB,QAAAnD,EAAA,sBAEjC,IAAMqC,EADS,MAAM1D,EAAYT,EAAM,OAAO,CAAC,EAE/C,QAAWkF,KAAMf,EACf,MAAMQ,EACJO,EACAvB,EACAC,EACAC,CACF,EAEFV,EAAQgB,CAAS,CACnB,GAZe5E,EAAA0F,EAAA,qBAcf,SAAeE,GAAkB,QAAArD,EAAA,sBAC/B,SAAesD,EAAkBrB,EAAqB,QAAAjC,EAAA,sBACpDiC,EAAK,KAAK,EACNA,EAAK,OAAS,IAChB,MAAMD,EAAmBC,CAAI,EAE7B,MAAMkB,EAAkB,CAE5B,GAPe1F,EAAA6F,EAAA,qBASf3E,EAAYT,EAAM,WAAW,CAAC,EAC3B,KAAYU,GAAWoB,EAAA,sBACtB,MAAMsD,EAAkB1E,CAAuB,CACjD,EAAC,EACA,MAAOE,GAAM,CACZwC,EAAOxC,CAAC,CACV,CAAC,EAECjB,EAAK,QAAQ,cACfA,EAAK,QAAQ,aAAa,CAE9B,GArBeJ,EAAA4F,EAAA,mBAuBfA,EAAgB,EAAE,KAAK,IAAM,CAE7B,CAAC,CACH,CAAC,CACH,GAgBA,eAAe3F,EAAgBE,EAAuC,CACpE,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,gIACF,EAGF,KAAK,oBAAsB,GAE3B,IAAMC,EAAO,KACbP,GAAS,QAAQ,IAAI,wBAAwB,EAC7CA,GAAS,QAAQ,KAAK,gBAAgB,EAEtC,KAAK,mBAAqB,KAC1B,KAAK,0BAA4B,CAAC,EAE9B,KAAK,MACP,KAAK,IAAI,MAAM,EACf,KAAK,IAAM,MAGb,IAAMiG,EAAU,UAAU,eAAe7F,CAAM,EAE/C6F,EAAQ,UAAY,IAAM,CACxB1F,EAAK,oBAAsB,GAC3BP,GAAS,QAAQ,QAAQ,gBAAgB,EACzCM,EAAS,CAAE,QAAS,EAAK,CAAC,CAC5B,EAEA2F,EAAQ,QAAWzE,GAAM,CACvBjB,EAAK,oBAAsB,GAC3B,QAAQ,MAAM,gCAAiCiB,CAAC,EAChDlB,EAAS,CAAE,QAAS,GAAO,MAAO2F,EAAQ,KAAM,CAAC,CACnD,EAEAA,EAAQ,UAAazE,GAAM,CAGzB,QAAQ,MACN,sEACAA,CACF,CACF,CACF,CACF,EA5uBarB,EAAAF,EAAA,+BA+uBb,SAASsB,EAAe2E,EAAS,CAC/B,IAAMpF,EAAc,CAAC,EAErB,OAAAoF,EAAQ,QAASC,GAAQ,CACvB,IAAMC,EAAcD,EAAI,MAAM,GAAG,EAEjC,GAAIC,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,QAAS,CAC1D,IAAM1E,EAAa0E,EAAY,CAAC,EAC1BzE,EAAU,SAASyE,EAAY,CAAC,CAAC,GAAK,EACtCC,EAAavF,EAAYY,CAAU,GAErC,CAAC2E,GAAc1E,EAAU0E,KAC3BvF,EAAYY,CAAU,EAAIC,GAGhC,CAAC,EACMb,CACT,CAjBSX,EAAAoB,EAAA,kBAmBT,SAASE,EAAmB6D,EAAO,CACjC,GAAI,CACF,OAAIA,GACW,KAAK,MAAMA,EAAM,KAAK,EACvB,cAAgB,IAIhC,OAAS9D,EAAP,CACA,eAAQ,MAAM,iCAAkCA,CAAC,EAC1C,IACT,CACF,CAZSrB,EAAAsB,EAAA,sBAcT,SAASmC,EAAYF,EAGnB,CACA,IAAIjB,EACE6D,EAAW,CAAC,EA8BlB,GA5BAC,EAAkB7C,CAAM,EAExBA,EAAO,QAAS4B,GAAU,CACxB,IAAMkB,EAAOlB,EAAM,KACb7B,EAAQ6B,EAAM,MACdrE,EAAOqE,EAAM,eACnB,GAAIkB,IAAS,OACX/D,EAAOgB,UACE+C,IAAS,OACdF,EAASrF,CAAI,EACfqF,EAASrF,CAAI,EAAE,WAAW,KAAKwC,CAAK,EAEpC6C,EAASrF,CAAI,EAAI,CACf,SAAU,KACV,WAAY,CAACwC,CAAK,CACpB,UAEO+C,IAAS,WACdF,EAASrF,CAAI,EACfqF,EAASrF,CAAI,EAAE,SAAWwC,EAE1B6C,EAASrF,CAAI,EAAI,CAAE,SAAUwC,EAAO,WAAY,CAAC,CAAE,MAGrD,OAAM,IAAI,MAAM,aAAa,CAEjC,CAAC,EAEG,CAAChB,EACH,MAAM,IAAI,MAAM,gDAAgD,EAGlE,MAAO,CAAE,KAAAA,EAAM,SAAA6D,CAAS,CAC1B,CAxCSnG,EAAAyD,EAAA,eA8GT,SAASC,EACP,CACE,YAAA4C,CACF,EAGAH,EACA9B,EACAC,EACA,CACAgC,EAAY,QAAQtG,EAAA,SAA4BuG,EAAgBtE,EAAG,CACjE,IAAMnB,EAAOyF,EAAe,KACtBC,EAAkBL,EAASrF,CAAI,EACrC,GAAI0F,EAAiB,CACnB,GAAI,CAACA,EAAgB,SACnB,MAAM,IAAI,MACR,mDAAmD1F,GACrD,EAEF,IAAMS,EAAaiF,EAAgB,SACnCA,EAAgB,SAAW,KAC3BF,EAAYrE,CAAC,EAAIV,EAEjB,IAAMkF,EAASnC,EAAgB,SAASxD,CAAI,EACtC4F,EAAkC1G,EAAA,IAAM,CAC5CH,GAAS4G,GAAU,QAAQ,IAAI,gBAAgB3F,GAAM,EACrD,IAAM6F,EAAO,CAAC,EACRC,EAAaJ,EAAgB,WACnC,OAAAI,EAAW,QAAQ5G,EAAA,SAAuBmF,EAAOlD,EAAG,CAC9CwE,IACFtB,EAAQ,KAAK,MAAMA,CAAK,EACpBd,IACFc,EAAQd,EAAiBvD,EAAMqE,CAAK,IAGxCA,EAAM,QAAS0B,GAAQ,CACrBF,EAAK,KAAKE,CAAG,CACf,CAAC,EACDD,EAAW3E,CAAC,EAAI,IAClB,EAXmB,gBAWlB,EACM0E,CACT,EAjBwC,mCAkBxCpF,EAAW,QAAUmF,EAEzB,EAlCoB,qBAkCnB,CACH,CA7CS1G,EAAA0D,EAAA,gBA+CT,SAASoD,EAAc3B,EAAyB,CAC9C,IAAMa,EAAMb,EAAM,IAElB,GAAIa,IAAQ,OAAQ,CAClBb,EAAM,KAAO,OACb,eACSa,EAAI,SAAS,GAAG,EAAG,CAC5B,IAAMC,EAAcD,EAAI,MAAM,GAAG,EACjC,GAAIC,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,QAAS,CAC1Dd,EAAM,KAAO,OACbA,EAAM,eAAiBc,EAAY,CAAC,EACpCd,EAAM,MAAQ,SAASc,EAAY,CAAC,EAAG,EAAE,EACzC,eACSA,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,WAAY,CACpEd,EAAM,KAAO,WACbA,EAAM,eAAiBc,EAAY,CAAC,EACpC,QAIJ,cAAQ,MAAM,iBAAiBD,GAAK,EAC9B,IAAI,MAAM,0CAA0C,CAC5D,CAtBShG,EAAA8G,EAAA,iBAyBT,SAAe1B,EACbD,EACAf,EAIAC,EAIAC,EACA,QAAA/B,EAAA,sBACAuE,EAAc3B,CAAK,EAEnB,IAAM4B,EAAS5B,EAAM,OAAS,OACxBsB,EAASnC,EAAgB,SAASa,EAAM,cAAc,EAEtD4B,GAAUN,IACdtB,EAAM,MAAQ,KAAK,MAAMA,EAAM,KAAe,GAE5C4B,GAAU,CAACN,IACTpC,EACFc,EAAM,MAAQd,EACZc,EAAM,eACNA,EAAM,KACR,EACSf,IACTe,EAAM,MAAQ,MAAMf,EAClBe,EAAM,eACNA,EAAM,KACR,GAGN,GAjCenF,EAAAoF,EAAA,mBAmCf,SAASjC,GAA0B,CAIjC,OAAO,KAAK,OAAO,EAAE,SAAS,EAAE,EAAE,UAAU,CAAC,CAC/C,CALSnD,EAAAmD,EAAA,mBAOT,SAASiD,EAAkB7C,EAA4B,CAGrDA,EAAO,KAAK,SAAUyD,EAAGC,EAAG,CAC1B,OAAQD,EAAE,OAAS,IAAMC,EAAE,OAAS,EACtC,CAAC,CACH,CANSjH,EAAAoG,EAAA,qBAQT,SAASzB,EAAgBH,EAAqB0C,EAAe,CAC3D,IAAMC,EAAgB,KAAK,MAAM3C,EAAK,OAAS0C,CAAK,EAC9CxC,EAAY,CAAC,EACf0C,EACAC,EACJ,QAASpF,EAAI,EAAGA,EAAIiF,EAAOjF,GAAK,EAC9BmF,EAAS5C,EAAK2C,EAAgBlF,CAAC,EAC/BoF,EAAS7C,EAAK2C,GAAiBlF,EAAI,EAAE,EACjCA,IAAM,EAERyC,EAAU,KAAK,YAAY,WAAW2C,EAAQ,EAAI,CAAC,EAC1CpF,IAAMiF,EAAQ,EAEvBxC,EAAU,KAAK,YAAY,WAAW0C,CAAM,CAAC,EAG7C1C,EAAU,KAAK,YAAY,MAAM0C,EAAQC,EAAQ,GAAO,EAAI,CAAC,EAGjE,OAAO3C,CACT,CApBS1E,EAAA2E,EAAA,mBAsBT,SAAezD,EAAe4E,EAAwB,QAAAvD,EAAA,sBACpD,OAAO,IAAI,QAAQ,CAACqB,EAASC,IAAW,CACtCiC,EAAQ,UAAazE,GAAM,CACzBuC,EAASvC,EAAE,OAAe,MAAW,CACvC,EACAyE,EAAQ,QAAU,IAAM,CACtBjC,EAAOiC,EAAQ,KAAK,CACtB,CACF,CAAC,CACH,GATe9F,EAAAkB,EAAA,eAWX,SAAW,QACb,OAAO,OAAO,OAAQ,CAAE,4BAAApB,CAA4B,CAAC",
  "names": ["DEBUG", "IncrementalIndexedDBAdapter", "options", "__name", "dbname", "getLokiCopy", "callback", "that", "finish", "updatePrevVersionIds", "didOverwrite", "tx", "store", "performSave", "maxChunkIds", "incremental", "chunkInfo", "name", "versionId", "error", "getAllKeysThenSave", "idbReqAsync", "result", "getMaxChunkIds", "e", "lokiChunkVersionId", "collection", "chunkId", "minId", "maxId", "idIndex", "firstDataPosition", "max", "min", "mid", "lastDataPosition", "i", "firstElement", "lastElement", "chunkData", "idbStore", "loki", "__async", "collectionVersionIds", "savedSize", "prepareCollection", "dirtyChunks", "lokiId", "prepareChunk", "maxChunkId", "j", "persistedMaxChunkId", "k", "deletedChunkName", "randomVersionId", "metadataChunk", "serializedMetadata", "value", "chunks", "n_chunks", "chunksToMap", "populateLoki", "idbVersionId", "resolve", "reject", "onError", "onSuccess", "openRequest", "oldVersion", "db", "versionChangeEvent", "deserializeChunkAsync", "deserializeChunk", "lazyCollections", "getMegachunksAsync", "keys", "megachunkCount", "keyRanges", "createKeyRanges", "allChunks", "megachunksReceived", "processMegachunkAsync", "megachunkIndex", "keyRange", "debugMsg", "megachunk", "chunk", "parseChunkAsync", "megachunkWaves", "megachunksPerWave", "requestMegachunk", "index", "wave", "getAllChunksAsync", "ch", "getAllKeysAsync", "onDidGetKeysAsync", "request", "allKeys", "key", "keySegments", "currentMax", "chunkMap", "sortChunksInPlace", "type", "collections", "collectionStub", "chunkCollection", "isLazy", "lokiDeserializeCollectionChunks", "data", "dataChunks", "doc", "classifyChunk", "isData", "a", "b", "count", "countPerRange", "minKey", "maxKey"]
}
